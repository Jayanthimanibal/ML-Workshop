import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

class NeuralNetwork:
    def _init_(self, layers):
        self.layers = layers
        self.weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]
        self.biases = [np.random.randn(y, 1) for y in layers[1:]]

    def forward_propagation(self, x):
        a = x
        activations = [a]
        zs = []

        for w, b in zip(self.weights, self.biases):
            z = np.dot(w, a) + b
            zs.append(z)
            a = sigmoid(z)
            activations.append(a)

        return activations, zs

    def backward_propagation(self, x, y, activations, zs):
        m = y.shape[1]  # Number of training examples

        delta = (activations[-1] - y) * sigmoid_derivative(zs[-1])
        dW = [np.dot(delta, activations[-2].T) / m]
        db = [np.sum(delta, axis=1, keepdims=True) / m]

        for l in range(2, len(self.layers)):
            delta = np.dot(self.weights[-l+1].T, delta) * sigmoid_derivative(zs[-l])
            dW.insert(0, np.dot(delta, activations[-l-1].T) / m)
            db.insert(0, np.sum(delta, axis=1, keepdims=True) / m)

        return dW, db

    def train(self, X, y, learning_rate, num_iterations):
        for i in range(num_iterations):
            activations, zs = self.forward_propagation(X)
            dW, db = self.backward_propagation(X, y, activations, zs)

            self.weights = [w - learning_rate * dw for w, dw in zip(self.weights, dW)]
            self.biases = [b - learning_rate * db for b, db in zip(self.biases, db)]

    def predict(self, X):
        activations, _ = self.forward_propagation(X)
        predictions = activations[-1]
        return predictions


layers = [input_size, hidden_size, output_size] 

nn = NeuralNetwork(layers)

learning_rate = 0.1
num_iterations = 1000
nn.train(X_train, y_train, learning_rate, num_iterations)

predictions = nn.predict(X_test)
